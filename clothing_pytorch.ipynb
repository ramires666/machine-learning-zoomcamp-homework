{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramires666/machine-learning-zoomcamp-homework/blob/main/clothing_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl36byRRU6zN",
        "outputId": "a03df2dc-d925-4a74-cee9-de3d0aec2190"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clothing-dataset-small'...\n",
            "remote: Enumerating objects: 3839, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3839/3839), 100.58 MiB | 26.38 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "BJ-AjlD_Vhlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "7DqXS4azVyeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lu3URFCkV6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 299\n",
        "\n",
        "train_gen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    shear_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    './clothing-dataset-small/train',\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "val_ds = train_gen.flow_from_directory(\n",
        "    './clothing-dataset-small/validation',\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X45gRvQtWBaT",
        "outputId": "b12e0686-21aa-4cb3-af64-33f2935adf37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3068 images belonging to 10 classes.\n",
            "Found 341 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    'xception_v4_1_{epoch:02d}_{val_accuracy:.3f}.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")"
      ],
      "metadata": {
        "id": "tuxJyGx7WFIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(input_size=150, learning_rate=0.01, size_inner=100,\n",
        "               droprate=0.5):\n",
        "\n",
        "    base_model = Xception(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(input_size, input_size, 3)\n",
        "    )\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 3))\n",
        "    base = base_model(inputs, training=False)\n",
        "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
        "\n",
        "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
        "    drop = keras.layers.Dropout(droprate)(inner)\n",
        "\n",
        "    outputs = keras.layers.Dense(10)(drop)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "REPu0MiuW8o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0005\n",
        "size = 100\n",
        "droprate = 0.2\n",
        "\n",
        "model = make_model(\n",
        "    input_size=input_size,\n",
        "    learning_rate=learning_rate,\n",
        "    size_inner=size,\n",
        "    droprate=droprate\n",
        ")\n",
        "\n",
        "history = model.fit(train_ds, epochs=50, validation_data=val_ds,\n",
        "                   callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_dAoTFkrWb-N",
        "outputId": "3d005db2-02ad-43c2-d63d-2f68519dc67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 843ms/step - accuracy: 0.5803 - loss: 1.3574 - val_accuracy: 0.8446 - val_loss: 0.5358\n",
            "Epoch 2/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 752ms/step - accuracy: 0.8104 - loss: 0.5661 - val_accuracy: 0.8622 - val_loss: 0.4536\n",
            "Epoch 3/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 755ms/step - accuracy: 0.8565 - loss: 0.4200 - val_accuracy: 0.8680 - val_loss: 0.4051\n",
            "Epoch 4/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 751ms/step - accuracy: 0.8690 - loss: 0.3779 - val_accuracy: 0.8475 - val_loss: 0.4545\n",
            "Epoch 5/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 754ms/step - accuracy: 0.8803 - loss: 0.3737 - val_accuracy: 0.8886 - val_loss: 0.3829\n",
            "Epoch 6/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 747ms/step - accuracy: 0.8879 - loss: 0.3280 - val_accuracy: 0.8915 - val_loss: 0.3561\n",
            "Epoch 7/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 735ms/step - accuracy: 0.9049 - loss: 0.2746 - val_accuracy: 0.8768 - val_loss: 0.3754\n",
            "Epoch 8/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 742ms/step - accuracy: 0.9022 - loss: 0.2970 - val_accuracy: 0.8680 - val_loss: 0.3714\n",
            "Epoch 9/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 782ms/step - accuracy: 0.9157 - loss: 0.2485 - val_accuracy: 0.8798 - val_loss: 0.3662\n",
            "Epoch 10/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 731ms/step - accuracy: 0.9151 - loss: 0.2451 - val_accuracy: 0.8798 - val_loss: 0.3581\n",
            "Epoch 11/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 744ms/step - accuracy: 0.9250 - loss: 0.2118 - val_accuracy: 0.8944 - val_loss: 0.3378\n",
            "Epoch 12/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 754ms/step - accuracy: 0.9185 - loss: 0.2204 - val_accuracy: 0.8710 - val_loss: 0.3618\n",
            "Epoch 13/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 751ms/step - accuracy: 0.9393 - loss: 0.1995 - val_accuracy: 0.8798 - val_loss: 0.3948\n",
            "Epoch 14/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 727ms/step - accuracy: 0.9462 - loss: 0.1721 - val_accuracy: 0.8856 - val_loss: 0.3618\n",
            "Epoch 15/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 748ms/step - accuracy: 0.9373 - loss: 0.1890 - val_accuracy: 0.8680 - val_loss: 0.3989\n",
            "Epoch 16/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 746ms/step - accuracy: 0.9409 - loss: 0.1816 - val_accuracy: 0.8680 - val_loss: 0.3690\n",
            "Epoch 17/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 775ms/step - accuracy: 0.9436 - loss: 0.1716 - val_accuracy: 0.8622 - val_loss: 0.3885\n",
            "Epoch 18/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 730ms/step - accuracy: 0.9514 - loss: 0.1591 - val_accuracy: 0.8651 - val_loss: 0.3662\n",
            "Epoch 19/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 746ms/step - accuracy: 0.9541 - loss: 0.1352 - val_accuracy: 0.8798 - val_loss: 0.3649\n",
            "Epoch 20/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 746ms/step - accuracy: 0.9571 - loss: 0.1301 - val_accuracy: 0.8739 - val_loss: 0.3814\n",
            "Epoch 21/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 751ms/step - accuracy: 0.9642 - loss: 0.1174 - val_accuracy: 0.8768 - val_loss: 0.3580\n",
            "Epoch 22/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 739ms/step - accuracy: 0.9533 - loss: 0.1355 - val_accuracy: 0.8680 - val_loss: 0.4106\n",
            "Epoch 23/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 754ms/step - accuracy: 0.9652 - loss: 0.1207 - val_accuracy: 0.8974 - val_loss: 0.3638\n",
            "Epoch 24/50\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 747ms/step - accuracy: 0.9583 - loss: 0.1287 - val_accuracy: 0.8651 - val_loss: 0.3968\n",
            "Epoch 25/50\n",
            "\u001b[1m 1/96\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:04\u001b[0m 11s/step - accuracy: 1.0000 - loss: 0.0427"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-952730532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit(train_ds, epochs=50, validation_data=val_ds,\n\u001b[0m\u001b[1;32m     13\u001b[0m                    callbacks=[checkpoint])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[1;32m    220\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5bcQLg6XBHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70150728"
      },
      "source": [
        "# Task\n",
        "Convert the provided TensorFlow/Keras code for an image classification model using Xception to PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644095d9"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Convert the data loading and preprocessing steps from `ImageDataGenerator` to PyTorch's `Dataset` and `DataLoader`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776969cc"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from PyTorch for data handling and image transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1520a78c"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33fb15c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a custom PyTorch Dataset class to load images from directories, apply transformations, and provide image-label pairs. Implement the `__len__` and `__getitem__` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1016cc"
      },
      "source": [
        "class ClothingDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = sorted(os.listdir(data_dir))\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "        for label_name in self.classes:\n",
        "            label_dir = os.path.join(data_dir, label_name)\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
        "                self.labels.append(self.class_to_idx[label_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012c9586"
      },
      "source": [
        "**Reasoning**:\n",
        "Define image transformations for both training and validation datasets, including preprocessing based on Xception's requirements. Then, create instances of the `ClothingDataset` and `DataLoader` for both sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6513076b"
      },
      "source": [
        "input_size = 224"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e911db4",
        "outputId": "eca6ee29-c93f-41c9-cfbe-7a7c17fefaf2"
      },
      "source": [
        "# Define image transformations for training with data augmentation and preprocessing\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    # image transformations\n",
        "    transforms.RandomRotation(10), # Equivalent to shear_range\n",
        "    transforms.RandomResizedCrop(input_size, scale=(0.9, 1.0)), # Equivalent to zoom_range\n",
        "    transforms.RandomHorizontalFlip(), # Equivalent to horizontal_flip\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # preprocessing\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Define image transformations for validation with preprocessing\n",
        "val_transforms = transforms.Compose([\n",
        "    # image transformations\n",
        "    transforms.Resize((input_size, input_size)), # Resize to the target size\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # preprocessing\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Create instances of the custom dataset for training and validation\n",
        "train_dataset = ClothingDataset(\n",
        "    data_dir='./clothing-dataset-small/train',\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "val_dataset = ClothingDataset(\n",
        "    data_dir='./clothing-dataset-small/validation',\n",
        "    transform=val_transforms\n",
        ")\n",
        "\n",
        "# Create DataLoaders for iterating through the datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches: 96\n",
            "Number of validation batches: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b66acb"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "### Subtask:\n",
        "Translate the Xception model and the custom layers from TensorFlow/Keras to PyTorch modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66f10c98"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from PyTorch for defining the neural network, including a pre-trained Xception model from `torchvision.models`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "626a87b4"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a0a9bfa"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the PyTorch model based on the structure of the Keras model: load a pre-trained Xception model, remove its original classification head, add a global average pooling layer, a dense inner layer with ReLU activation and dropout, and a final dense output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb885a4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Create an instance of the PyTorch model and move it to the appropriate device (GPU if available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d280072"
      },
      "source": [
        "## Model Definition (using MobileNetV2)\n",
        "\n",
        "### Subtask:\n",
        "Translate the model definition to use MobileNetV2 from `torchvision.models`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e68cda7"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from PyTorch for defining the neural network and MobileNetV2 from `torchvision.models`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2dff9f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the PyTorch model using a pre-trained MobileNetV2 model, replace its classifier, and add the necessary layers for the clothing classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f17e0c5"
      },
      "source": [
        "class ClothingClassifierMobileNet(nn.Module):\n",
        "    def __init__(self, size_inner=100, droprate=0.2, num_classes=10):\n",
        "        super(ClothingClassifierMobileNet, self).__init__()\n",
        "        # Load pre-trained MobileNetV2 model\n",
        "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Replace the original classifier\n",
        "        # The original classifier in MobileNetV2 starts with a Conv2d layer\n",
        "        # followed by a Linear layer. We need to replace the entire classifier.\n",
        "        # The input features to the classifier are 1280 for MobileNetV2.\n",
        "        self.base_model.classifier = nn.Identity() # Remove the original classifier\n",
        "\n",
        "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.inner = nn.Linear(1280, size_inner) # 1280 is the number of output features from MobileNetV2 features\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(droprate)\n",
        "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model.features(x) # Access features attribute for MobileNetV2\n",
        "        x = self.global_avg_pooling(x)\n",
        "        x = torch.flatten(x, 1) # Flatten the output from pooling\n",
        "        x = self.inner(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b9e6c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Create an instance of the MobileNetV2-based PyTorch model and move it to the appropriate device (GPU if available)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "size = 32 # Corresponds to size_inner in the TensorFlow code\n",
        "droprate = 0.2 # Corresponds to droprate in the TensorFlow code"
      ],
      "metadata": {
        "id": "hVt_JU-bJAhS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a567ac4b",
        "outputId": "4e934f5c-8128-4d5b-cf27-6b56658c593a"
      },
      "source": [
        "model = ClothingClassifierMobileNet(size_inner=size, droprate=droprate, num_classes=len(train_dataset.classes))\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model is on: {device}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92c55e5e"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Adapt the training loop, including the optimizer, loss function, and model training steps, from Keras's `model.fit` to a PyTorch training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc211341"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary PyTorch modules for optimization and loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28d1997d"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f36ded9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the optimizer and the loss function for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ac3fddf"
      },
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# Use CrossEntropyLoss for classification with logits\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ab61f9"
      },
      "source": [
        "## Model Checkpointing\n",
        "\n",
        "### Subtask:\n",
        "Implement model checkpointing in PyTorch equivalent to the Keras `ModelCheckpoint` callback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4618bf5c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the training loop, including iterating over epochs and batches, calculating loss and accuracy, performing backpropagation, and updating model weights. Also, include the logic for saving the best model based on validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d078751b",
        "outputId": "8f144aa0-6e7f-4362-cbeb-75031941ce43"
      },
      "source": [
        "num_epochs = 65\n",
        "best_val_accuracy = 0.0\n",
        "checkpoint_path = 'mobilenet_v2_v1_{epoch:02d}_{val_accuracy:.3f}.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during validation\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total_predictions += labels.size(0)\n",
        "            val_correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct_predictions / val_total_predictions\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Checkpoint logic\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Format the checkpoint filename with epoch and accuracy\n",
        "        current_checkpoint_path = checkpoint_path.format(epoch=epoch+1, val_accuracy=val_accuracy)\n",
        "        torch.save(model.state_dict(), current_checkpoint_path)\n",
        "        print(f'Checkpoint saved to {current_checkpoint_path}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/65, Train Loss: 1.4000, Train Accuracy: 0.5831\n",
            "Epoch 1/65, Val Loss: 0.6715, Val Accuracy: 0.8211\n",
            "Checkpoint saved to mobilenet_v2_v1_01_0.821.pth\n",
            "Epoch 2/65, Train Loss: 0.5745, Train Accuracy: 0.8325\n",
            "Epoch 2/65, Val Loss: 0.3538, Val Accuracy: 0.8827\n",
            "Checkpoint saved to mobilenet_v2_v1_02_0.883.pth\n",
            "Epoch 3/65, Train Loss: 0.3631, Train Accuracy: 0.8947\n",
            "Epoch 3/65, Val Loss: 0.3028, Val Accuracy: 0.9003\n",
            "Checkpoint saved to mobilenet_v2_v1_03_0.900.pth\n",
            "Epoch 4/65, Train Loss: 0.2622, Train Accuracy: 0.9257\n",
            "Epoch 4/65, Val Loss: 0.2495, Val Accuracy: 0.9150\n",
            "Checkpoint saved to mobilenet_v2_v1_04_0.915.pth\n",
            "Epoch 5/65, Train Loss: 0.1822, Train Accuracy: 0.9485\n",
            "Epoch 5/65, Val Loss: 0.2480, Val Accuracy: 0.9179\n",
            "Checkpoint saved to mobilenet_v2_v1_05_0.918.pth\n",
            "Epoch 6/65, Train Loss: 0.1398, Train Accuracy: 0.9671\n",
            "Epoch 6/65, Val Loss: 0.2555, Val Accuracy: 0.9150\n",
            "Epoch 7/65, Train Loss: 0.1258, Train Accuracy: 0.9645\n",
            "Epoch 7/65, Val Loss: 0.2076, Val Accuracy: 0.9326\n",
            "Checkpoint saved to mobilenet_v2_v1_07_0.933.pth\n",
            "Epoch 8/65, Train Loss: 0.0909, Train Accuracy: 0.9795\n",
            "Epoch 8/65, Val Loss: 0.2556, Val Accuracy: 0.9208\n",
            "Epoch 9/65, Train Loss: 0.0869, Train Accuracy: 0.9778\n",
            "Epoch 9/65, Val Loss: 0.2349, Val Accuracy: 0.9238\n",
            "Epoch 10/65, Train Loss: 0.0751, Train Accuracy: 0.9824\n",
            "Epoch 10/65, Val Loss: 0.2094, Val Accuracy: 0.9355\n",
            "Checkpoint saved to mobilenet_v2_v1_10_0.935.pth\n",
            "Epoch 11/65, Train Loss: 0.0530, Train Accuracy: 0.9883\n",
            "Epoch 11/65, Val Loss: 0.2924, Val Accuracy: 0.9150\n",
            "Epoch 12/65, Train Loss: 0.0580, Train Accuracy: 0.9863\n",
            "Epoch 12/65, Val Loss: 0.2572, Val Accuracy: 0.9296\n",
            "Epoch 13/65, Train Loss: 0.0445, Train Accuracy: 0.9889\n",
            "Epoch 13/65, Val Loss: 0.2426, Val Accuracy: 0.9208\n",
            "Epoch 14/65, Train Loss: 0.0629, Train Accuracy: 0.9837\n",
            "Epoch 14/65, Val Loss: 0.2820, Val Accuracy: 0.9238\n",
            "Epoch 15/65, Train Loss: 0.0605, Train Accuracy: 0.9844\n",
            "Epoch 15/65, Val Loss: 0.2593, Val Accuracy: 0.9150\n",
            "Epoch 16/65, Train Loss: 0.0416, Train Accuracy: 0.9866\n",
            "Epoch 16/65, Val Loss: 0.2374, Val Accuracy: 0.9355\n",
            "Epoch 17/65, Train Loss: 0.0435, Train Accuracy: 0.9873\n",
            "Epoch 17/65, Val Loss: 0.2656, Val Accuracy: 0.9267\n",
            "Epoch 18/65, Train Loss: 0.0524, Train Accuracy: 0.9863\n",
            "Epoch 18/65, Val Loss: 0.2234, Val Accuracy: 0.9267\n",
            "Epoch 19/65, Train Loss: 0.0351, Train Accuracy: 0.9896\n",
            "Epoch 19/65, Val Loss: 0.2082, Val Accuracy: 0.9355\n",
            "Epoch 20/65, Train Loss: 0.0316, Train Accuracy: 0.9912\n",
            "Epoch 20/65, Val Loss: 0.2497, Val Accuracy: 0.9238\n",
            "Epoch 21/65, Train Loss: 0.0434, Train Accuracy: 0.9883\n",
            "Epoch 21/65, Val Loss: 0.3074, Val Accuracy: 0.9179\n",
            "Epoch 22/65, Train Loss: 0.0406, Train Accuracy: 0.9892\n",
            "Epoch 22/65, Val Loss: 0.3144, Val Accuracy: 0.9150\n",
            "Epoch 23/65, Train Loss: 0.0430, Train Accuracy: 0.9883\n",
            "Epoch 23/65, Val Loss: 0.3334, Val Accuracy: 0.9150\n",
            "Epoch 24/65, Train Loss: 0.0293, Train Accuracy: 0.9919\n",
            "Epoch 24/65, Val Loss: 0.2155, Val Accuracy: 0.9384\n",
            "Checkpoint saved to mobilenet_v2_v1_24_0.938.pth\n",
            "Epoch 25/65, Train Loss: 0.0290, Train Accuracy: 0.9905\n",
            "Epoch 25/65, Val Loss: 0.2753, Val Accuracy: 0.9355\n",
            "Epoch 26/65, Train Loss: 0.0293, Train Accuracy: 0.9909\n",
            "Epoch 26/65, Val Loss: 0.3060, Val Accuracy: 0.9355\n",
            "Epoch 27/65, Train Loss: 0.0183, Train Accuracy: 0.9958\n",
            "Epoch 27/65, Val Loss: 0.2530, Val Accuracy: 0.9326\n",
            "Epoch 28/65, Train Loss: 0.0260, Train Accuracy: 0.9941\n",
            "Epoch 28/65, Val Loss: 0.2921, Val Accuracy: 0.9296\n",
            "Epoch 29/65, Train Loss: 0.0198, Train Accuracy: 0.9951\n",
            "Epoch 29/65, Val Loss: 0.3529, Val Accuracy: 0.9179\n",
            "Epoch 30/65, Train Loss: 0.0122, Train Accuracy: 0.9980\n",
            "Epoch 30/65, Val Loss: 0.3035, Val Accuracy: 0.9267\n",
            "Epoch 31/65, Train Loss: 0.0155, Train Accuracy: 0.9971\n",
            "Epoch 31/65, Val Loss: 0.2660, Val Accuracy: 0.9355\n",
            "Epoch 32/65, Train Loss: 0.0232, Train Accuracy: 0.9928\n",
            "Epoch 32/65, Val Loss: 0.3949, Val Accuracy: 0.9150\n",
            "Epoch 33/65, Train Loss: 0.0252, Train Accuracy: 0.9928\n",
            "Epoch 33/65, Val Loss: 0.3222, Val Accuracy: 0.9238\n",
            "Epoch 34/65, Train Loss: 0.0368, Train Accuracy: 0.9899\n",
            "Epoch 34/65, Val Loss: 0.3925, Val Accuracy: 0.9179\n",
            "Epoch 35/65, Train Loss: 0.0459, Train Accuracy: 0.9873\n",
            "Epoch 35/65, Val Loss: 0.5366, Val Accuracy: 0.8827\n",
            "Epoch 36/65, Train Loss: 0.0592, Train Accuracy: 0.9860\n",
            "Epoch 36/65, Val Loss: 0.4091, Val Accuracy: 0.9003\n",
            "Epoch 37/65, Train Loss: 0.0283, Train Accuracy: 0.9932\n",
            "Epoch 37/65, Val Loss: 0.3618, Val Accuracy: 0.9150\n",
            "Epoch 38/65, Train Loss: 0.0268, Train Accuracy: 0.9928\n",
            "Epoch 38/65, Val Loss: 0.2997, Val Accuracy: 0.9179\n",
            "Epoch 39/65, Train Loss: 0.0139, Train Accuracy: 0.9974\n",
            "Epoch 39/65, Val Loss: 0.2752, Val Accuracy: 0.9208\n",
            "Epoch 40/65, Train Loss: 0.0097, Train Accuracy: 0.9984\n",
            "Epoch 40/65, Val Loss: 0.2968, Val Accuracy: 0.9179\n",
            "Epoch 41/65, Train Loss: 0.0149, Train Accuracy: 0.9951\n",
            "Epoch 41/65, Val Loss: 0.2961, Val Accuracy: 0.9296\n",
            "Epoch 42/65, Train Loss: 0.0129, Train Accuracy: 0.9954\n",
            "Epoch 42/65, Val Loss: 0.3506, Val Accuracy: 0.9179\n",
            "Epoch 43/65, Train Loss: 0.0227, Train Accuracy: 0.9935\n",
            "Epoch 43/65, Val Loss: 0.2957, Val Accuracy: 0.9179\n",
            "Epoch 44/65, Train Loss: 0.0270, Train Accuracy: 0.9932\n",
            "Epoch 44/65, Val Loss: 0.3425, Val Accuracy: 0.9150\n",
            "Epoch 45/65, Train Loss: 0.0145, Train Accuracy: 0.9961\n",
            "Epoch 45/65, Val Loss: 0.3434, Val Accuracy: 0.9179\n",
            "Epoch 46/65, Train Loss: 0.0090, Train Accuracy: 0.9967\n",
            "Epoch 46/65, Val Loss: 0.3334, Val Accuracy: 0.9062\n",
            "Epoch 47/65, Train Loss: 0.0196, Train Accuracy: 0.9945\n",
            "Epoch 47/65, Val Loss: 0.2969, Val Accuracy: 0.9296\n",
            "Epoch 48/65, Train Loss: 0.0335, Train Accuracy: 0.9912\n",
            "Epoch 48/65, Val Loss: 0.3787, Val Accuracy: 0.9179\n",
            "Epoch 49/65, Train Loss: 0.0299, Train Accuracy: 0.9905\n",
            "Epoch 49/65, Val Loss: 0.4126, Val Accuracy: 0.9208\n",
            "Epoch 50/65, Train Loss: 0.0288, Train Accuracy: 0.9912\n",
            "Epoch 50/65, Val Loss: 0.3564, Val Accuracy: 0.9208\n",
            "Epoch 51/65, Train Loss: 0.0177, Train Accuracy: 0.9948\n",
            "Epoch 51/65, Val Loss: 0.2983, Val Accuracy: 0.9326\n",
            "Epoch 52/65, Train Loss: 0.0135, Train Accuracy: 0.9954\n",
            "Epoch 52/65, Val Loss: 0.4098, Val Accuracy: 0.9062\n",
            "Epoch 53/65, Train Loss: 0.0167, Train Accuracy: 0.9951\n",
            "Epoch 53/65, Val Loss: 0.3564, Val Accuracy: 0.9120\n",
            "Epoch 54/65, Train Loss: 0.0365, Train Accuracy: 0.9896\n",
            "Epoch 54/65, Val Loss: 0.3804, Val Accuracy: 0.9238\n",
            "Epoch 55/65, Train Loss: 0.0427, Train Accuracy: 0.9886\n",
            "Epoch 55/65, Val Loss: 0.2455, Val Accuracy: 0.9384\n",
            "Epoch 56/65, Train Loss: 0.0255, Train Accuracy: 0.9928\n",
            "Epoch 56/65, Val Loss: 0.2683, Val Accuracy: 0.9267\n",
            "Epoch 57/65, Train Loss: 0.0213, Train Accuracy: 0.9932\n",
            "Epoch 57/65, Val Loss: 0.3064, Val Accuracy: 0.9208\n",
            "Epoch 58/65, Train Loss: 0.0193, Train Accuracy: 0.9938\n",
            "Epoch 58/65, Val Loss: 0.3906, Val Accuracy: 0.9150\n",
            "Epoch 59/65, Train Loss: 0.0171, Train Accuracy: 0.9948\n",
            "Epoch 59/65, Val Loss: 0.3914, Val Accuracy: 0.9179\n",
            "Epoch 60/65, Train Loss: 0.0231, Train Accuracy: 0.9945\n",
            "Epoch 60/65, Val Loss: 0.2965, Val Accuracy: 0.9238\n",
            "Epoch 61/65, Train Loss: 0.0096, Train Accuracy: 0.9977\n",
            "Epoch 61/65, Val Loss: 0.3224, Val Accuracy: 0.9267\n",
            "Epoch 62/65, Train Loss: 0.0141, Train Accuracy: 0.9967\n",
            "Epoch 62/65, Val Loss: 0.3116, Val Accuracy: 0.9472\n",
            "Checkpoint saved to mobilenet_v2_v1_62_0.947.pth\n",
            "Epoch 63/65, Train Loss: 0.0098, Train Accuracy: 0.9967\n",
            "Epoch 63/65, Val Loss: 0.3865, Val Accuracy: 0.9267\n",
            "Epoch 64/65, Train Loss: 0.0107, Train Accuracy: 0.9964\n",
            "Epoch 64/65, Val Loss: 0.3821, Val Accuracy: 0.9179\n",
            "Epoch 65/65, Train Loss: 0.0099, Train Accuracy: 0.9964\n",
            "Epoch 65/65, Val Loss: 0.4058, Val Accuracy: 0.9208\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964712d6"
      },
      "source": [
        "## Export PyTorch model to ONNX\n",
        "\n",
        "### Subtask:\n",
        "Convert the trained PyTorch model to the ONNX format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7618babd"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the best saved PyTorch model, set it to evaluation mode, and export it to an ONNX file using `torch.onnx.export`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7gxvfjOJT9f",
        "outputId": "4dbbe7af-3741-4bed-b4d1-f2b969705e2b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.20.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best checkpoint file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('mobilenet_v2_v1_*.pth')\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "print(f\"Loading the best model from: {latest_file}\")\n",
        "\n",
        "# latest_file = 'mobilenet_0.889.pth'\n",
        "\n",
        "model = ClothingClassifierMobileNet(size_inner=size, droprate=droprate, num_classes=len(train_dataset.classes))\n",
        "model.load_state_dict(torch.load(latest_file))\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDwvAI_YRODz",
        "outputId": "7d48d879-72e8-4ea2-803f-e86bcc4ed93c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the best model from: mobilenet_v2_v1_62_0.947.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClothingDataset(\n",
        "    data_dir='./clothing-dataset-small/test',\n",
        "    transform=val_transforms,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "ogjLNYcMR6qh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    break"
      ],
      "metadata": {
        "id": "HwQ0g-7-RS3J"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPUQc8VmRlm6",
        "outputId": "78c9a208-17d3-4727-e003-1137bed0f2cb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dMQFLTZRxyM",
        "outputId": "e3008d96-f17b-480f-e6ca-0fe4ad7e8e1c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 4, 3, 0, 2, 3, 4, 6, 2, 0, 5, 2, 8, 9, 5, 6, 3, 7, 1, 2, 2, 6, 5, 3,\n",
              "        4, 4, 9, 6, 6, 9, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(inputs)\n",
        "pred_labels = torch.max(outputs, 1).indices\n",
        "(labels == pred_labels).sum() / len(pred_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX8wFlf-SMCG",
        "outputId": "ae2564ef-962e-4b78-8d42-4be157db250a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8750, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-image-helper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "3VUeDg1WSrRg",
        "outputId": "400907ff-8d48-4095-c9c0-fcd980cf2fec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-image-helper\n",
            "  Downloading keras_image_helper-0.0.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting numpy>=2.3.2 (from keras-image-helper)\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=11.3.0 in /usr/local/lib/python3.12/dist-packages (from keras-image-helper) (11.3.0)\n",
            "Downloading keras_image_helper-0.0.2-py3-none-any.whl (5.4 kB)\n",
            "Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, keras-image-helper\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-image-helper-0.0.2 numpy-2.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "87d5e3fb65b34846a380f5268e1506a4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==2.0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "nQhFYBsatHDQ",
        "outputId": "fb77ce56-ce8e-4724-a29a-649785beca4d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0.2\n",
            "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-image-helper 0.0.2 requires numpy>=2.3.2, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0cc96a7dabc445419184c7a377d5589c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_image_helper import create_preprocessor"
      ],
      "metadata": {
        "id": "CNM9xr3hTLci"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pytorch_style(X):\n",
        "    # X: shape (1, 299, 299, 3), dtype=float32, values in [0, 255]\n",
        "    X = X / 255.0\n",
        "\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "\n",
        "    # Convert NHWC → NCHW\n",
        "    # from (batch, height, width, channels) → (batch, channels, height, width)\n",
        "    X = X.transpose(0, 3, 1, 2)\n",
        "\n",
        "    # Normalize\n",
        "    X = (X - mean) / std\n",
        "\n",
        "    return X.astype(np.float32)"
      ],
      "metadata": {
        "id": "hW7aIJBbWt4I"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_image_helper import create_preprocessor"
      ],
      "metadata": {
        "id": "2y-2qMyiWoYE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = create_preprocessor(preprocess_pytorch_style, target_size=(224, 224))"
      ],
      "metadata": {
        "id": "8CLIhyAzS--V"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://bit.ly/mlbookcamp-pants'\n",
        "X = preprocessor.from_url(url)"
      ],
      "metadata": {
        "id": "rYgTt5ikTr3t"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXVJuOnZTyQY",
        "outputId": "f03507f0-00d6-41ed-f9d7-fe7c805055d7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.Tensor(X).to(device)"
      ],
      "metadata": {
        "id": "OBlt_bz7USbn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.array(model(X).data[0].cpu())"
      ],
      "metadata": {
        "id": "rfqFCo-jUJPM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"dress\",\n",
        "    \"hat\",\n",
        "    \"longsleeve\",\n",
        "    \"outwear\",\n",
        "    \"pants\",\n",
        "    \"shirt\",\n",
        "    \"shoes\",\n",
        "    \"shorts\",\n",
        "    \"skirt\",\n",
        "    \"t-shirt\",\n",
        "]\n",
        "\n",
        "dict(zip(classes, pred.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Uzx4wlUtcC",
        "outputId": "19d878e2-1158-4480-e346-2d747aef55ce"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dress': -0.05191972106695175,\n",
              " 'hat': -5.399651050567627,\n",
              " 'longsleeve': -1.9374825954437256,\n",
              " 'outwear': -1.8340309858322144,\n",
              " 'pants': 14.948799133300781,\n",
              " 'shirt': -6.481940746307373,\n",
              " 'shoes': 0.08216299116611481,\n",
              " 'shorts': 0.5064080357551575,\n",
              " 'skirt': -4.075653553009033,\n",
              " 't-shirt': -4.513818740844727}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1163df",
        "outputId": "52d9f6b1-d82c-4100-f301-863593a0956d"
      },
      "source": [
        "# Define dummy input for ONNX export\n",
        "# The input shape should match the input shape of your model (batch_size, channels, height, width)\n",
        "# Use a batch size of 1 for simplicity when exporting\n",
        "dummy_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
        "\n",
        "# Export the model to ONNX format\n",
        "onnx_path = \"clothing_classifier_mobilenet_v2_latest.onnx\"\n",
        "\n",
        "# Install onnxscript if not already installed\n",
        "!pip install onnxscript\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,                     # PyTorch Model\n",
        "    dummy_input,               # Dummy input tensor\n",
        "    onnx_path,                 # Path to save the ONNX model\n",
        "    verbose=True,              # Print export details\n",
        "    input_names=['input'],     # Input layer name\n",
        "    output_names=['output'],   # Output layer name\n",
        "    dynamic_axes={             # Dynamic batch size\n",
        "        'input' : {0 : 'batch_size'},\n",
        "        'output' : {0 : 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model exported to {onnx_path}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.12/dist-packages (0.5.6)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.1.12)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (5.29.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2614452395.py:12: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `ClothingClassifierMobileNet([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `ClothingClassifierMobileNet([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 104 of general pattern rewrite rules.\n",
            "Model exported to clothing_classifier_mobilenet_v2_latest.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfS3FY62ZVA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}